version: '3'
services:
  server:
    build:
      context: ../LLMConversationalAgent
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    networks:
      - llm-network

  client:
    build:
      context: ../LLMConversationalAgentClient
      dockerfile: Dockerfile
    environment:
      - SERVER_URL=http://server:8080
      - OLLAMA_HOST=http://host.docker.internal:11434
    ports:
      - "8081:8081"
    volumes:
      - conversation-logs:/app/resources  # Map the resources directory to a local logs directory
    depends_on:
      - server
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge

volumes:
  conversation-logs: