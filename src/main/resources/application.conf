http {
  interface = "0.0.0.1"
  port = 8081
}

server{
  url = "http://localhost:8080"
}

llm {
  maxResponseLength = 1000
  timeout = 30s
}

ollama {
  host = "http://localhost:11434"
  model = "llama3.2:1b"
  request-timeout-seconds = 500
}

conversation {
  max-turns = 5  # Maximum number of back-and-forth turns in a conversation
}